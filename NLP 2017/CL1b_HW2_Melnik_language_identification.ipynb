{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программа распознавания языка. На текстах википедии проверяем два метода: словарный метод и N-граммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг1. Скачать тренировочные тексты из Вики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#функция скачивает из Википедии n текстов на языке lang\n",
    "def get_texts_for_lang(lang, n=10):\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# скачиваем по 100 статей для каждого языка\n",
    "#kk - казахский, uk - украинский, be - белорусский, fr - французский\n",
    "\n",
    "wiki_texts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Мәдениет ауылдық округі\n",
      "kk 99\n"
     ]
    }
   ],
   "source": [
    "wiki_texts['kk'] = get_texts_for_lang('kk', 100)\n",
    "print('kk', len(wiki_texts['kk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Костерін\n",
      "Skipping page Інтерв'ю (значення)\n",
      "Skipping page Флореску\n",
      "Skipping page Симонів\n",
      "Skipping page Складан\n",
      "Skipping page Торчин\n",
      "Skipping page Євсєєв\n",
      "uk 93\n"
     ]
    }
   ],
   "source": [
    "wiki_texts['uk'] = get_texts_for_lang('uk', 100)\n",
    "print('uk', len(wiki_texts['uk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Грынявічы\n",
      "Skipping page Канюхі\n",
      "Skipping page Амасія\n",
      "be 97\n"
     ]
    }
   ],
   "source": [
    "wiki_texts['be'] = get_texts_for_lang('be', 100)\n",
    "print('be', len(wiki_texts['be']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Delbet\n",
      "Skipping page Croix de guerre\n",
      "Skipping page Lahic\n",
      "Skipping page Association Sportive de Corbeil-Essonnes Canoë-Kayak\n",
      "Skipping page Liu Zhihua\n",
      "Skipping page Conil\n",
      "Skipping page Cottingham\n",
      "fr 93\n"
     ]
    }
   ],
   "source": [
    "wiki_texts['fr'] = get_texts_for_lang('fr', 100)\n",
    "print('fr', len(wiki_texts['fr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аврамово (Кырджали облысы)\n",
      "Аврамово (болг. Аврамово) — Болгариядағы ауыл. Кырджали облысына қарасты Добрич қауымының құрамына кіреді.\n",
      "2010 жылғы мәліметтер бойынша тұрғындарының саны 5 адамды құрайды.\n",
      "Пошта индексі — 6797. Телефон коды — 07442. ЕКАТТЕ коды — 62.\n",
      "\n",
      "\n",
      " Картада \n",
      "bgmaps.com электронды картада орналасуы\n",
      "emaps.bg электронды картада орналасуы\n",
      "Google электронды картада орналасуы\n",
      "\n",
      "\n",
      " Сыртқы сілтемелер \n",
      "Тұрғын статистикасы  (болг.)\n",
      "Іоанніс Ралліс\n",
      "Іоа́нніс Ра́лліс (грец. Ιωάννης Ράλλης; 1878 — 26 жовтня 1946) — грецький політик, глава лялькового уряду часів німецької окупації Греції.\n",
      "\n",
      "\n",
      " Життєпис \n",
      "Належав до відомої політичної династії (його батько також був прем'єр-міністром). 1936 року об'єднався з колишнім диктатором Георгіосом Кондилісом, якого король Георг II усунув від влади за кілька днів після своєї реставрації. Був засуджений за зраду до довічного ув'язнення, помер у в'язниці.\n",
      "Його син, Георгіос, 1947 року видав книгу «Іоанніс Ралліс мовить з могили», де було посмертно опубліковано записки Ралліса, в яких він висловлював розкуту за свою зраду. Згодом його син також очолив грецький уряд (1980–1981).\n",
      "\n",
      "\n",
      " Джерела \n",
      "Βιογραφική Εγκυκλοπαίδεια του Νεωτέρου Ελληνισμού 1830-2010 - Αρχεία Ελληνικής Βιογραφίας, Εκδόσεις Μέτρον, τόμος Γ΄ (гр.)\n",
      "Ота Гельстэд\n",
      "Ота Гельстэд (дацк.: Otto Gelsted; сапр.: Эйнар Епесен, Jeppesen, 4 лістапада 1888, Мідэльфарт — 22 снежня 1968, Капенгаген) — дацкі паэт. Член Камуністычнай партыі Даніі (з 1929).\n",
      "У юнацтве захапляўся філасофіяй і эстэтыкай левага экспрэсіянізму (трактат \"Экспрэсіянізм\", 1919). У 1920 выйшаў зборнік вершаў \"Вечныя рэчы\". У 1931 апублікаваў цыкл антынацысцкіх вершаў \"Да святла\". \"Эмігранцкія вершы\" (1945), напісаныя ў Швецыі падчас эміграцыі, поўныя нянавісці да гітлераўскіх акупантаў і захаплення барацьбітамі Супраціву. У зборніках \"Гады свабоды\" (1947), \"Устань і запалі святло\" (1948), \"Песні дзён халоднай вайны\" (1952), \"Смерць у ваннай\" (1955) і іншых Гельстэд перасцерагае супраць адраджэння нацызму. Аўтар зборніка артыкулаў \"Добры дзень, жыццё\" (1958), зборніка вершаў \"Ніколі яшчэ дзень не быў так светлы\" (1959). Зборнік \"Вершы сонечнага берага\" (1961) прысвечаны Грэцыі. Пераклаў на дацкую мову \"Іліяду\" і \"Адысею\".\n",
      "\n",
      "\n",
      " Літаратура \n",
      "Lene Nordin, Otto Gelsteds standpunkt : sammenhængen mellem filosofi og digtning i Otto Gelsteds forfatterskab indtil 1940, Museum Tusculanum, 1983. (Studier fra sprog- og oldtidsforskning / udgivet af Det Filologisk-Historiske Samfund, nr. 303). ISBN 87-88073-35-1.\n",
      "Jonas Nutcracker\n",
      "Jonas Nutcracker est un personnage de fiction qui apparaît dans la série de bande dessinée Les Tuniques bleues.\n",
      "\n",
      "\n",
      " Biographie fictive \n",
      "Jonas Nutcracker est un militaire nordiste pendant la guerre de Sécession. Il est le père de Barnaby Nutcracker.\n",
      "Avant de s'engager dans l'armée, il était fermier en Pennsylvanie.\n",
      "Son fils se retrouvant interné à la suite de l'explosion des réserves de poudre d'où il ressort mutilé et complètement sonné, ne reconnaissant plus son père et ne prononçant plus que le mot \" Yessir\", il s'engage alors volontairement dans l'armée afin d'avoir l'autorisation de le voir.\n",
      "\n",
      "\n",
      " Œuvres où le personnage apparaît \n",
      "Les Bleus en folie : 32e album de la série de bande dessinée Les Tuniques bleues de Willy Lambil (dessins) et de Raoul Cauvin (scénario) publié en 1991. Il croit reconnaitre en Blutch son fils Barnaby Nutcracker qui lui ressemble comme deux gouttes d'eau ( à la différence que Barnaby a une énorme cicatrice sur le torse).\n",
      " Portail de la bande dessinée\n",
      " Portail du western\n"
     ]
    }
   ],
   "source": [
    "# распечатаем по 1 тексту\n",
    "print(wiki_texts['kk'][0])\n",
    "print(wiki_texts['uk'][0])\n",
    "print(wiki_texts['be'][0])\n",
    "print(wiki_texts['fr'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод 1. Словарный метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2. Составить частотные списки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "def tokenize(text): #токенизация по пробелам\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_kk = collections.defaultdict(lambda: 0)\n",
    "corpus_kk = wiki_texts['kk']\n",
    "for article in corpus_kk:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_kk[word] += 1\n",
    "\n",
    "for word in sorted(freqs_kk, key=lambda w: freqs_kk[w], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_kk[word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_uk = collections.defaultdict(lambda: 0)\n",
    "corpus_uk = wiki_texts['uk']\n",
    "for article in corpus_uk:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_uk[word] += 1\n",
    "\n",
    "for word in sorted(freqs_uk, key=lambda w: freqs_uk[w], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_uk[word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_be = collections.defaultdict(lambda: 0)\n",
    "corpus_be = wiki_texts['be']\n",
    "for article in corpus_be:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_be[word] += 1\n",
    "\n",
    "for word in sorted(freqs_be, key=lambda w: freqs_be[w], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_be[word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_fr = collections.defaultdict(lambda: 0)\n",
    "corpus_fr = wiki_texts['fr']\n",
    "for article in corpus_fr:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_fr[word] += 1\n",
    "\n",
    "for word in sorted(freqs_fr, key=lambda w: freqs_fr[w], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_fr[word], word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почистим словари от слов, которые встречаются больше, чем в 1 языке. Таким образом, у нас останутся только лингвоспецифические слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs_list = [freqs_kk, freqs_uk, freqs_be, freqs_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(freqs_list)):\n",
    "    words = list(freqs_list[i].keys())\n",
    "    k = 0\n",
    "    while k != len(words):\n",
    "        word = words[k]\n",
    "        for j in range(0, len(freqs_list)):\n",
    "            if freqs_list[j] != freqs_list[i]:\n",
    "                if word in freqs_list[j]:\n",
    "                    del freqs_list[j][word]\n",
    "                    flag = 'T'\n",
    "        if flag == 'T':\n",
    "            del freqs_list[i][word]\n",
    "            flag = 'F'\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем из них словари по 100 самых частотных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def freqs_sort(d, n):\n",
    "    freqs_sort = {}\n",
    "    words_sort = sorted(d, key=lambda w: d[w], reverse=True)[:n]\n",
    "    for i in words_sort:\n",
    "        freqs_sort[i] = d[i]\n",
    "    return freqs_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs_kk_sort = freqs_sort(freqs_kk, 100)\n",
    "freqs_uk_sort = freqs_sort(freqs_uk, 100)\n",
    "freqs_be_sort = freqs_sort(freqs_be, 100)\n",
    "freqs_fr_sort = freqs_sort(freqs_fr, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3. Определить язык"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим текст на токены и смотрим, в каком частотном словаре больше слов из текста "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lng_id_words(text, freqs, lngs): #freqs - все имеющиеся частотные словари\n",
    "    count_freqs = [0]*len(freqs)\n",
    "    text_tokenized = tokenize(text.replace('\\n', '').lower())\n",
    "    for i in text_tokenized:\n",
    "        for j in range(0, len(freqs)):\n",
    "            if i in freqs[j]:\n",
    "                count_freqs[j] += 1\n",
    "    return lngs[count_freqs.index(max(count_freqs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 4. Скачать тестовые тексты из Википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_texts_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Седан\n",
      "kk 99\n"
     ]
    }
   ],
   "source": [
    "wiki_texts_test['kk'] = get_texts_for_lang('kk', 100)\n",
    "print('kk', len(wiki_texts_test['kk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Фрессін\n",
      "Skipping page Вовківська сільська рада\n",
      "Skipping page Чиж (значення)\n",
      "Skipping page Підлісне (Белебеївський район)\n",
      "Skipping page Оболонська вулиця\n",
      "Skipping page Микитенко Ольга Леонтіївна\n",
      "Skipping page Ягорі\n",
      "Skipping page Шеховцов Анатолій Федорович\n",
      "uk 92\n"
     ]
    }
   ],
   "source": [
    "wiki_texts_test['uk'] = get_texts_for_lang('uk', 100)\n",
    "print('uk', len(wiki_texts_test['uk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Гарбачы\n",
      "Skipping page Лік, значэнні\n",
      "Skipping page Ніва\n",
      "Skipping page Пракопавічы\n",
      "Skipping page Грэск\n",
      "be 95\n"
     ]
    }
   ],
   "source": [
    "wiki_texts_test['be'] = get_texts_for_lang('be', 100)\n",
    "print('be', len(wiki_texts_test['be']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page ADY\n",
      "Skipping page Missisicabi\n",
      "Skipping page Ligne 108\n",
      "Skipping page Oberglatt\n",
      "Skipping page Pas sur la bouche (homonymie)\n",
      "Skipping page Luini (homonymie)\n",
      "fr 94\n"
     ]
    }
   ],
   "source": [
    "wiki_texts_test['fr'] = get_texts_for_lang('fr', 100)\n",
    "print('fr', len(wiki_texts_test['fr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 5. Проверить метод на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs_words_sort_list = [freqs_kk_sort, freqs_uk_sort, freqs_be_sort, freqs_fr_sort]\n",
    "lngs = ['Kazakh', 'Ukranian', 'Belorussian', 'French']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_kk_test = wiki_texts_test['kk']\n",
    "kk = 0\n",
    "for article in corpus_kk_test:\n",
    "    answer = lng_id_words(article, freqs_words_sort_list, lngs)\n",
    "    if answer == 'Kazakh':\n",
    "        kk += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность классификации казахских текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_kk = kk / len(wiki_texts_test['kk'])\n",
    "accuracy_kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_uk_test = wiki_texts_test['uk']\n",
    "uk = 0\n",
    "for article in corpus_uk_test:\n",
    "    answer = lng_id_words(article, freqs_words_sort_list, lngs)\n",
    "    if answer == 'Ukranian':\n",
    "        uk += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность классификации украинских текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_uk = uk / len(wiki_texts_test['uk'])\n",
    "accuracy_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_be_test = wiki_texts_test['be']\n",
    "be = 0\n",
    "for article in corpus_be_test:\n",
    "    answer = lng_id_words(article, freqs_words_sort_list, lngs)\n",
    "    if answer == 'Belorussian':\n",
    "        be += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789473684210527"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_be = be / len(wiki_texts_test['be'])\n",
    "accuracy_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_fr_test = wiki_texts_test['fr']\n",
    "fr = 0\n",
    "for article in corpus_fr_test:\n",
    "    answer = lng_id_words(article, freqs_words_sort_list, lngs)\n",
    "    if answer == 'French':\n",
    "        fr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fr = fr / len(wiki_texts_test['fr'])\n",
    "accuracy_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суммарная точность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947368421052631"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (kk + uk + be + fr) / len(wiki_texts_test['kk'] + wiki_texts_test['uk'] + wiki_texts_test['be'] + wiki_texts_test['fr'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод 2. N-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая преобразовывает строку text в массив N-грамм заданной длины N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text, N=3):\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим частотные словари n-грамм аналогично первому методу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_grams_kk = collections.defaultdict(lambda: 0)\n",
    "\n",
    "for article in corpus_kk:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        freqs_grams_kk[ngram] += 1\n",
    "\n",
    "for ngram in sorted(freqs_grams_kk, key=lambda n: freqs_grams_kk[n], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_grams_kk[ngram], ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_grams_uk = collections.defaultdict(lambda: 0)\n",
    "\n",
    "for article in corpus_uk:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        freqs_grams_uk[ngram] += 1\n",
    "\n",
    "for ngram in sorted(freqs_grams_uk, key=lambda n: freqs_grams_uk[n], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_grams_uk[ngram], ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_grams_be = collections.defaultdict(lambda: 0)\n",
    "\n",
    "for article in corpus_be:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        freqs_grams_be[ngram] += 1\n",
    "\n",
    "for ngram in sorted(freqs_grams_be, key=lambda n: freqs_grams_be[n], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_grams_be[ngram], ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs_grams_fr = collections.defaultdict(lambda: 0)\n",
    "\n",
    "for article in corpus_fr:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        freqs_grams_fr[ngram] += 1\n",
    "\n",
    "for ngram in sorted(freqs_grams_fr, key=lambda n: freqs_grams_fr[n], reverse=True)[:50]:\n",
    "    print('{}\\t{}'.format(freqs_grams_fr[ngram], ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почистим аналогично первому методу списки от N-грамм, которые встречаются больше, чем в 1 языке. Таким образом, у нас останутся только лингвоспецифические N-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs_grams_list = [freqs_grams_kk, freqs_grams_uk, freqs_grams_be, freqs_grams_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(freqs_grams_list)):\n",
    "    grams = list(freqs_grams_list[i].keys())\n",
    "    k = 0\n",
    "    while k != len(grams):\n",
    "        gram = grams[k]\n",
    "        for j in range(0, len(freqs_grams_list)):\n",
    "            if freqs_grams_list[j] != freqs_grams_list[i]:\n",
    "                if gram in freqs_grams_list[j]:\n",
    "                    del freqs_grams_list[j][gram]\n",
    "                    flag = 'T'\n",
    "        if flag == 'T':\n",
    "            del freqs_grams_list[i][gram]\n",
    "            flag = 'F'\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем из них словари по 100 самых частотных N-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs_grams_kk_sort = freqs_sort(freqs_grams_kk, 100)\n",
    "freqs_grams_uk_sort = freqs_sort(freqs_grams_uk, 100)\n",
    "freqs_grams_be_sort = freqs_sort(freqs_grams_be, 100)\n",
    "freqs_grams_fr_sort = freqs_sort(freqs_grams_fr, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 6. Определить язык"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lng_id_grams(text, freqs, lngs): #freqs - все имеющиеся частотные словари\n",
    "    count_freqs = [0]*len(freqs)\n",
    "    text_grammed = make_ngrams(text.replace('\\n', '').lower())\n",
    "    for i in text_grammed:\n",
    "        for j in range(0, len(freqs)):\n",
    "            if i in freqs[j]:\n",
    "                count_freqs[j] += 1\n",
    "    return lngs[count_freqs.index(max(count_freqs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 7. Проверить метод на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs_grams_sort_list = [freqs_grams_kk_sort, freqs_grams_uk_sort, freqs_grams_be_sort, freqs_grams_fr_sort]\n",
    "lngs = ['Kazakh', 'Ukranian', 'Belorussian', 'French']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kk = 0\n",
    "for article in corpus_kk_test:\n",
    "    answer = lng_id_grams(article, freqs_grams_sort_list, lngs)\n",
    "    if answer == 'Kazakh':\n",
    "        kk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_kk = kk / len(wiki_texts_test['kk'])\n",
    "accuracy_kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uk = 0\n",
    "for article in corpus_uk_test:\n",
    "    answer = lng_id_grams(article, freqs_grams_sort_list, lngs)\n",
    "    if answer == 'Ukranian':\n",
    "        uk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_uk = uk / len(wiki_texts_test['uk'])\n",
    "accuracy_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "be = 0\n",
    "for article in corpus_be_test:\n",
    "    answer = lng_id_grams(article, freqs_grams_sort_list, lngs)\n",
    "    if answer == 'Belorussian':\n",
    "        be += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_be = be / len(wiki_texts_test['be'])\n",
    "accuracy_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr = 0\n",
    "for article in corpus_fr_test:\n",
    "    answer = lng_id_grams(article, freqs_grams_sort_list, lngs)\n",
    "    if answer == 'French':\n",
    "        fr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_be = be / len(wiki_texts_test['be'])\n",
    "accuracy_be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суммарная точност метода N-грамм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (kk + uk + be + fr) / len(wiki_texts_test['kk'] + wiki_texts_test['uk'] + wiki_texts_test['be'] + wiki_texts_test['fr'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программа, определяющая язык текста двумя способами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lng_id_words_grams(text, lngs, freqs_words=freqs_words_sort_list, freqs_grams=freqs_grams_sort_list):\n",
    "    count_freqs_words = [0]*len(freqs_words)\n",
    "    count_freqs_grams = [0]*len(freqs_grams)\n",
    "    text_grammed = make_ngrams(text.replace('\\n', '').lower())\n",
    "    for i in text_grammed:\n",
    "        for j in range(0, len(freqs_grams)):\n",
    "            if i in freqs_grams[j]:\n",
    "                count_freqs_grams[j] += 1\n",
    "    text_tokenized = tokenize(text.replace('\\n', '').lower())\n",
    "    for i in text_tokenized:\n",
    "        for j in range(0, len(freqs_words)):\n",
    "            if i in freqs_words[j]:\n",
    "                count_freqs_words[j] += 1\n",
    "    return lngs[count_freqs_grams.index(max(count_freqs_grams))], lngs[count_freqs_words.index(max(count_freqs_words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод N-грамм работает лучше. \n",
    "На тестовой выборке в 500 документов метод частотных слов дает точность 0.9947368421052631, \n",
    "а метод N-грамм - 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Kazakh')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Belorussian')\n",
      "('Belorussian', 'Kazakh')\n"
     ]
    }
   ],
   "source": [
    "for article in corpus_be_test:\n",
    "    print(lng_id_words_grams(article, lngs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
