{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import trigrams\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "import unittest\n",
    "from unittest import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiParser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_articles(self, start):\n",
    "        end_punct = set(['.', '!', '?'])\n",
    "        exclude = set(string.punctuation) - set('-')\n",
    "        article = Wikipedia().article(start)\n",
    "        links = article.links\n",
    "        list_of_strings = []\n",
    "        for link in links:\n",
    "            linked_article = Wikipedia().article(link)\n",
    "            if linked_article is not None:\n",
    "                s = plaintext(linked_article.source)\n",
    "                s = s.encode('ascii', 'ignore')\n",
    "                s = ' '.join(re.split('\\s+', s))\n",
    "                s_clear = ''\n",
    "                for i, symbol in enumerate(s):\n",
    "                    if symbol in exclude:\n",
    "                        if i < len(s)-1:\n",
    "                            if symbol in end_punct and s[i+1] == ' ' and s[i+2].isupper() == True:\n",
    "                                s_clear += symbol\n",
    "                        else:\n",
    "                            s_clear += symbol\n",
    "                    else:\n",
    "                        s_clear += symbol\n",
    "                s = s_clear.lower()\n",
    "                s = ' '.join(re.split('\\s+', s))\n",
    "                list_of_strings.append(s)\n",
    "        return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextStatistics:\n",
    "    def __init__(self, articles):\n",
    "        self.list = articles\n",
    "        pass\n",
    "    \n",
    "    def get_top_3grams(self, n, use_idf=False):\n",
    "        dict_trigrams = {}\n",
    "        new_list = []\n",
    "        sentences = []\n",
    "        \n",
    "        for article in self.list:\n",
    "            new = ''.join([i for i in article if (not i.isdigit())])\n",
    "            sentences += re.split('[\\.?!]', new)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_trigrams = list(trigrams(sentence))\n",
    "            for sentence_trigram in sentence_trigrams:\n",
    "                sentence_trigram_join = ''.join(sentence_trigram)\n",
    "                if sentence_trigram_join in dict_trigrams:\n",
    "                    dict_trigrams[sentence_trigram_join] += 1\n",
    "                else:\n",
    "                    dict_trigrams[sentence_trigram_join] = 1\n",
    "                               \n",
    "        if use_idf == True:\n",
    "            for gram in dict_trigrams:\n",
    "                gram_in_sentences = [gram in i for i in sentences].count(True)\n",
    "                idf = math.log(len(sentences) / float(gram_in_sentences))\n",
    "                dict_trigrams[gram] *= idf\n",
    "        items_sorted = sorted(dict_trigrams.items(), key=lambda pair:(pair[1], pair[0]), reverse=True)\n",
    "        items_sorted_trigrs = [i[0] for i in items_sorted]\n",
    "        items_sorted_freqs = [i[1] for i in items_sorted]\n",
    "        list_of_3grams_in_descending_order_by_freq = items_sorted_trigrs[:n]\n",
    "        list_of_their_corresponding_freq = items_sorted_freqs[:n]\n",
    "        return (list_of_3grams_in_descending_order_by_freq, list_of_their_corresponding_freq)\n",
    "    \n",
    "    def get_top_words(self, n, use_idf=False):\n",
    "        dict_words = {}\n",
    "        pos_articles = set(['a', 'an', 'the'])\n",
    "        some_stop_words = ['and', 'are', 'that', 'to', 'this', 'which']\n",
    "        pos_prepositions = set(['aboard', 'about', 'above', 'across', 'afore', 'after', 'against', 'along', 'amid', 'amidst', 'among', 'amongst', 'around', 'as', 'aside', 'aslant', 'astride', 'at', 'athwart', 'atop', 'between', 'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'betwixt', 'beyond', 'by', 'circa', 'despite','down', 'except', 'for', 'from', 'in', 'inside', 'into', 'like', 'near', 'neath', 'next', 'of', 'off', 'on', 'opposite', 'out', 'outside', 'over', 'per', 'through', 'till'', toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'with', 'without'])\n",
    "        sentences = []\n",
    "        \n",
    "        for article in self.list:\n",
    "            new = ''.join([i for i in article if not i.isdigit()])\n",
    "            sentences += re.split('[\\.?!]', new)\n",
    "            \n",
    "        for sentence in sentences:\n",
    "            sentence_words = [word for word in sentence.split() if ((not (word in pos_articles)) and (not (word in pos_prepositions)))]\n",
    "\n",
    "            for sentence_word in sentence_words:\n",
    "                if sentence_word in dict_words:\n",
    "                    dict_words[sentence_word] += 1\n",
    "                else:\n",
    "                    dict_words[sentence_word] = 1\n",
    "                    \n",
    "        if use_idf == True:\n",
    "            for word in dict_words:\n",
    "                word_in_sentences = [word in i for i in sentences].count(True)\n",
    "                idf = math.log(len(sentences) / float(word_in_sentences))\n",
    "                dict_words[word] *= idf\n",
    "        items_sorted = sorted(dict_words.items(), key=lambda pair:(pair[1], pair[0]), reverse=True)\n",
    "        items_sorted_words = [i[0] for i in items_sorted]\n",
    "        items_sorted_freqs = [i[1] for i in items_sorted]\n",
    "        list_of_words_in_descending_order_by_freq = items_sorted_words[:n]\n",
    "        list_of_their_corresponding_freq = items_sorted_freqs[:n]\n",
    "        return (list_of_words_in_descending_order_by_freq, list_of_their_corresponding_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def show_results(self):\n",
    "        exclude = set(string.punctuation) - set('-')\n",
    "        parser = WikiParser()\n",
    "        articles = parser.get_articles(\"Natural_language_processing\")\n",
    "        stat = TextStatistics(articles)\n",
    "        top20_3grams = stat.get_top_3grams(20, use_idf=True)\n",
    "        top20_words = stat.get_top_words(20, use_idf=True)\n",
    "        print( color.BOLD + \"TOP-20 3GRAMS\" + color.END)\n",
    "        for i, gram in enumerate(top20_3grams[0]):\n",
    "            print(gram + '\\t' + str(top20_3grams[1][i]))\n",
    "        print(color.BOLD + \"TOP-20 WORDS\" + color.END)\n",
    "        for i, word in enumerate(top20_words[0]):\n",
    "            print(word + '\\t' + str(top20_words[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTOP-20 3GRAMS\u001b[0m\n",
      " th\t26694.1034624\n",
      "the\t25118.1174635\n",
      "ion\t23684.8945818\n",
      "he \t23598.7322497\n",
      "tio\t22673.3454391\n",
      "on \t21935.59116\n",
      "ing\t21835.395426\n",
      "ati\t21525.6462717\n",
      "ng \t21399.1946103\n",
      " in\t20773.7228908\n",
      "al \t20582.479183\n",
      " co\t19945.6734235\n",
      "es \t19820.862613\n",
      " of\t19785.6478591\n",
      " an\t19587.5737507\n",
      "of \t19466.1147752\n",
      "ent\t19207.9106379\n",
      "and\t18791.5024732\n",
      "ed \t18790.1833479\n",
      "nd \t18755.8273771\n",
      "\u001b[1mTOP-20 WORDS\u001b[0m\n",
      "and\t15354.3993144\n",
      "to\t10431.68797\n",
      "that\t9721.25388441\n",
      "language\t8171.29804143\n",
      "are\t7640.78132006\n",
      "this\t6225.90009612\n",
      "which\t5867.61027672\n",
      "english\t5469.71161134\n",
      "displaystyle\t5434.0494509\n",
      "languages\t5289.60882739\n",
      "is\t5241.76504468\n",
      "was\t5224.02694613\n",
      "speech\t5190.83901859\n",
      "such\t4911.24408281\n",
      "words\t4875.19195919\n",
      "have\t4766.99684711\n",
      "retrieved\t4765.79436402\n",
      "also\t4606.42395273\n",
      "can\t4514.84714811\n",
      "used\t4485.54916452\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment()\n",
    "exp.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextStatisticsTest(TestCase):\n",
    "\n",
    "    def test_empty(self):\n",
    "        articles = TextStatistics([])\n",
    "        self.assertEqual(articles.get_top_3grams(5, False), ([], []))\n",
    "        self.assertEqual(articles.get_top_words(5, False), ([], []))\n",
    "    \n",
    "    def test_without_idf(self):\n",
    "        articles = TextStatistics(['i want to sleep', 'i love eat', 'eat! pray! love!'])\n",
    "        self.assertEqual(articles.get_top_3grams(3, False), (['ove', 'lov', 'eat'], [2, 2, 2]))\n",
    "        self.assertEqual(articles.get_top_words(3, False), (['love', 'i', 'eat'], [2, 2, 2]))\n",
    "        \n",
    "    def test_with_idf(self):\n",
    "        articles = TextStatistics(['as if', 'yes if']) #as - стоп-слово\n",
    "        self.assertEqual(articles.get_top_3grams(1, True), (['yes'], [0.6931471805599453]))\n",
    "        self.assertEqual(articles.get_top_words(2, True), (['yes', 'if'], [0.6931471805599453, 0.0]))\n",
    "    \n",
    "    def test_all(self):\n",
    "        articles = TextStatistics(['no if', 'yes if'])\n",
    "        d = (['yes', 's i', 'o i' , 'no ', 'es ', ' if'], [0.6931471805599453, 0.6931471805599453, 0.6931471805599453,  0.6931471805599453, 0.6931471805599453, 0.0])\n",
    "        self.assertEqual(articles.get_top_3grams(6, True), d)\n",
    "        self.assertEqual(articles.get_top_words(3, True), (['yes', 'no', 'if'], [0.6931471805599453, 0.6931471805599453, 0.0]))                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.030s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case = TextStatisticsTest()\n",
    "suite = TestLoader().loadTestsFromModule(case)\n",
    "TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
